{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeFQqj96GeaMD9WgLkZvx1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achal1008/CODTECH-Task5/blob/main/Task5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ED13nLgoSy",
        "outputId": "6a79675e-37fa-401f-cd1c-f31a7ab3fd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Download NLTK data files (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data\n",
        "text_data = [\n",
        "    \"Natural Language Processing is an exciting field of AI.\",\n",
        "    \"Machine learning techniques are widely used in NLP.\",\n",
        "    \"Tokenization splits text into smaller units called tokens.\",\n",
        "    \"Stop words removal and stemming simplify text preprocessing.\"\n",
        "]\n",
        "\n",
        "# Create a DataFrame for better organization\n",
        "df = pd.DataFrame({'Text': text_data})\n",
        "print(\"Sample Dataset:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp56nonAg4mX",
        "outputId": "4818225d-3d3a-4aa5-b3a8-8e44bc4f260e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Dataset:\n",
            "                                                Text\n",
            "0  Natural Language Processing is an exciting fie...\n",
            "1  Machine learning techniques are widely used in...\n",
            "2  Tokenization splits text into smaller units ca...\n",
            "3  Stop words removal and stemming simplify text ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Download NLTK data files (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab') # This line is added to download the missing data\n",
        "\n",
        "# Sample text data\n",
        "text_data = [\n",
        "    \"Natural Language Processing is an exciting field of AI.\",\n",
        "    \"Machine learning techniques are widely used in NLP.\",\n",
        "    \"Tokenization splits text into smaller units called tokens.\",\n",
        "    \"Stop words removal and stemming simplify text preprocessing.\"\n",
        "]\n",
        "\n",
        "# Create a DataFrame for better organization\n",
        "df = pd.DataFrame({'Text': text_data})\n",
        "print(\"Sample Dataset:\")\n",
        "print(df)\n",
        "\n",
        "# Initialize preprocessing tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    # Stemming\n",
        "    stemmed = [stemmer.stem(word) for word in tokens]\n",
        "    # Lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return {\n",
        "        'original': tokens,\n",
        "        'stemmed': stemmed,\n",
        "        'lemmatized': lemmatized\n",
        "    }\n",
        "\n",
        "# Apply preprocessing\n",
        "df['Processed'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "# Display processed results\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"\\nOriginal: {row['Text']}\")\n",
        "    print(f\"Tokens: {row['Processed']['original']}\")\n",
        "    print(f\"Stemmed: {row['Processed']['stemmed']}\")\n",
        "    print(f\"Lemmatized: {row['Processed']['lemmatized']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5vTLIuQhFGn",
        "outputId": "9e04b882-14f3-4b18-ef84-1b9a499796ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Dataset:\n",
            "                                                Text\n",
            "0  Natural Language Processing is an exciting fie...\n",
            "1  Machine learning techniques are widely used in...\n",
            "2  Tokenization splits text into smaller units ca...\n",
            "3  Stop words removal and stemming simplify text ...\n",
            "\n",
            "Original: Natural Language Processing is an exciting field of AI.\n",
            "Tokens: ['natural', 'language', 'processing', 'exciting', 'field', 'ai']\n",
            "Stemmed: ['natur', 'languag', 'process', 'excit', 'field', 'ai']\n",
            "Lemmatized: ['natural', 'language', 'processing', 'exciting', 'field', 'ai']\n",
            "\n",
            "Original: Machine learning techniques are widely used in NLP.\n",
            "Tokens: ['machine', 'learning', 'techniques', 'widely', 'used', 'nlp']\n",
            "Stemmed: ['machin', 'learn', 'techniqu', 'wide', 'use', 'nlp']\n",
            "Lemmatized: ['machine', 'learning', 'technique', 'widely', 'used', 'nlp']\n",
            "\n",
            "Original: Tokenization splits text into smaller units called tokens.\n",
            "Tokens: ['tokenization', 'splits', 'text', 'smaller', 'units', 'called', 'tokens']\n",
            "Stemmed: ['token', 'split', 'text', 'smaller', 'unit', 'call', 'token']\n",
            "Lemmatized: ['tokenization', 'split', 'text', 'smaller', 'unit', 'called', 'token']\n",
            "\n",
            "Original: Stop words removal and stemming simplify text preprocessing.\n",
            "Tokens: ['stop', 'words', 'removal', 'stemming', 'simplify', 'text', 'preprocessing']\n",
            "Stemmed: ['stop', 'word', 'remov', 'stem', 'simplifi', 'text', 'preprocess']\n",
            "Lemmatized: ['stop', 'word', 'removal', 'stemming', 'simplify', 'text', 'preprocessing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare tokenized data for Word2Vec\n",
        "tokenized_sentences = [word_tokenize(text.lower()) for text in text_data]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Explore Word2Vec\n",
        "print(\"\\nWord Embedding for 'text':\")\n",
        "print(word2vec_model.wv['text'])\n",
        "\n",
        "# Similar words to a given word\n",
        "print(\"\\nWords similar to 'text':\")\n",
        "print(word2vec_model.wv.most_similar('text'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnFW_1g3jCeZ",
        "outputId": "c452b2b3-3401-4c81-d36d-064a9c5d87ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Embedding for 'text':\n",
            "[-8.6238794e-03  3.6745830e-03  5.1925741e-03  5.7528294e-03\n",
            "  7.4716806e-03 -6.1675208e-03  1.1122372e-03  6.0473173e-03\n",
            " -2.8424340e-03 -6.1763953e-03 -4.1011415e-04 -8.3771348e-03\n",
            " -5.5925283e-03  7.1041961e-03  3.3601974e-03  7.2282390e-03\n",
            "  6.7970352e-03  7.5313943e-03 -3.7887702e-03 -5.6490797e-04\n",
            "  2.3522321e-03 -4.5127845e-03  8.3881933e-03 -9.8569123e-03\n",
            "  6.7674574e-03  2.9204448e-03 -4.9379929e-03  4.3964330e-03\n",
            " -1.7378879e-03  6.7073135e-03  9.9660438e-03 -4.3653608e-03\n",
            " -5.9867528e-04 -5.6973062e-03  3.8511637e-03  2.7912559e-03\n",
            "  6.8960432e-03  6.1060768e-03  9.5363026e-03  9.2762588e-03\n",
            "  7.9019908e-03 -6.9915722e-03 -9.1613289e-03 -3.5356585e-04\n",
            " -3.1016364e-03  7.8910720e-03  5.9392545e-03 -1.5460115e-03\n",
            "  1.5096061e-03  1.7863335e-03  7.8231776e-03 -9.5131425e-03\n",
            " -2.0779530e-04  3.4683782e-03 -9.3811052e-04  8.3859861e-03\n",
            "  9.0123536e-03  6.5332088e-03 -7.0981367e-04  7.7074431e-03\n",
            " -8.5421922e-03  3.2021625e-03 -4.6302811e-03 -5.0876872e-03\n",
            "  3.5870702e-03  5.3749378e-03  7.7658035e-03 -5.7647368e-03\n",
            "  7.4274582e-03  6.6291061e-03 -3.7186840e-03 -8.7451115e-03\n",
            "  5.4409741e-03  6.5108868e-03 -7.9289509e-04 -6.7087305e-03\n",
            " -7.0916647e-03 -2.4973936e-03  5.1414720e-03 -3.6649965e-03\n",
            " -9.3789874e-03  3.8237874e-03  4.8924452e-03 -6.4337300e-03\n",
            "  1.2084100e-03 -2.0726915e-03  2.9791898e-05 -9.8878918e-03\n",
            "  2.6936128e-03 -4.7501069e-03  1.0868317e-03 -1.5757972e-03\n",
            "  2.1955890e-03 -7.8791250e-03 -2.7103056e-03  2.6597639e-03\n",
            "  5.3472174e-03 -2.3858133e-03 -9.5105981e-03  4.5093396e-03]\n",
            "\n",
            "Words similar to 'text':\n",
            "[('stop', 0.1892170011997223), ('into', 0.1890406310558319), ('processing', 0.16084004938602448), ('simplify', 0.15935294330120087), ('is', 0.13726061582565308), ('smaller', 0.1278192698955536), ('language', 0.12304278463125229), ('in', 0.08543874323368073), ('ai', 0.06804239749908447), ('natural', 0.03826490417122841)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# Load GloVe embeddings (50-dimensional vectors as an example)\n",
        "glove_path = \"glove.6B.50d.txt\"\n",
        "embeddings = {}\n",
        "\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings[word] = vector\n",
        "\n",
        "# Example: Get embedding for a word\n",
        "word = \"text\"\n",
        "print(f\"\\nGloVe Embedding for '{word}':\\n{embeddings.get(word)}\")\n",
        "\n",
        "# Find similar words (requires cosine similarity implementation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2id98Z7BjIy8",
        "outputId": "c76403db-7ac0-45af-80a0-8cb3be1b84c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-12 06:27:01--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-12-12 06:27:01--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-12-12 06:27:01--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-12-12 06:29:40 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "\n",
            "GloVe Embedding for 'text':\n",
            "[ 0.32615    0.36686   -0.0074905 -0.37553    0.66715    0.21646\n",
            " -0.19801   -1.1001    -0.42221    0.10574   -0.31292    0.50953\n",
            "  0.55775    0.12019    0.31441   -0.25043   -1.0637    -1.3213\n",
            "  0.87798   -0.24627    0.27379   -0.51092    0.49324    0.52243\n",
            "  1.1636    -0.75323   -0.48053   -0.11259   -0.54595   -0.83921\n",
            "  2.9825    -1.1916    -0.51958   -0.39365   -0.1419    -0.026977\n",
            "  0.66296    0.16574   -1.1681     0.14443    1.6305    -0.17216\n",
            " -0.17436   -0.01049   -0.17794    0.93076    1.0381     0.94266\n",
            " -0.14805   -0.61109  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(text_data)\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out()))\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()))\n"
      ],
      "metadata": {
        "id": "k-WoZQ_fkC2t",
        "outputId": "6fcf134e-f0af-42da-8fdc-a9916276f205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag of Words Matrix:\n",
            "   ai  an  and  are  called  exciting  field  in  into  is  ...  stemming  \\\n",
            "0   1   1    0    0       0         1      1   0     0   1  ...         0   \n",
            "1   0   0    0    1       0         0      0   1     0   0  ...         0   \n",
            "2   0   0    0    0       1         0      0   0     1   0  ...         0   \n",
            "3   0   0    1    0       0         0      0   0     0   0  ...         1   \n",
            "\n",
            "   stop  techniques  text  tokenization  tokens  units  used  widely  words  \n",
            "0     0           0     0             0       0      0     0       0      0  \n",
            "1     0           1     0             0       0      0     1       1      0  \n",
            "2     0           0     1             1       1      1     0       0      0  \n",
            "3     1           0     1             0       0      0     0       0      1  \n",
            "\n",
            "[4 rows x 32 columns]\n",
            "\n",
            "TF-IDF Matrix:\n",
            "         ai        an       and       are    called  exciting     field  \\\n",
            "0  0.333333  0.333333  0.000000  0.000000  0.000000  0.333333  0.333333   \n",
            "1  0.000000  0.000000  0.000000  0.353553  0.000000  0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.362224  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.362224  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "         in      into        is  ...  stemming      stop  techniques  \\\n",
            "0  0.000000  0.000000  0.333333  ...  0.000000  0.000000    0.000000   \n",
            "1  0.353553  0.000000  0.000000  ...  0.000000  0.000000    0.353553   \n",
            "2  0.000000  0.362224  0.000000  ...  0.000000  0.000000    0.000000   \n",
            "3  0.000000  0.000000  0.000000  ...  0.362224  0.362224    0.000000   \n",
            "\n",
            "       text  tokenization    tokens     units      used    widely     words  \n",
            "0  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000      0.000000  0.000000  0.000000  0.353553  0.353553  0.000000  \n",
            "2  0.285582      0.362224  0.362224  0.362224  0.000000  0.000000  0.000000  \n",
            "3  0.285582      0.000000  0.000000  0.000000  0.000000  0.000000  0.362224  \n",
            "\n",
            "[4 rows x 32 columns]\n"
          ]
        }
      ]
    }
  ]
}